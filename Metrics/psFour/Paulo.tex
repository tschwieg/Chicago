\documentclass[12pt]{paper}

\usepackage{Schwieg}
\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usepackage{minted}

\begin{document}

\section*{Question 2}

\subsection*{a}

In class we proved that there is no perfect colinearity in a matrix $W$ if and only if $\exV{WW'}$ is invertible. So, if we show that $\exV{WW'}$ is invertible, then we have shown that there is no perfect colinearity in $W$.

Assume, to contrary, that $\exV{WW'}$ is not invertible. This means that there exists $c\ne0$ such that $\exV{WW'}c=0$. But then we have:
\begin{equation}
\begin{split}
0&=c'\exV{WW'}c\\
&=c'\exV{AXX'A'}c=\exV{c'AXX'A'c}\\
&=\exV{d'XX'd}=\exV{(d'X)^2},
\end{split}
\end{equation}

\noindent where $d:=A'c\ne0$, because $c\ne0$ and $A$ is invertible. But because $(d'X)^2>0$ always, $\exV{(d'X)^2}=0$ implies $\Pr(d'X=0)=1$, contradicting the assumption that there is no perfect colinearity in $X$. Thus we cannot have $\exV{WW'}c=0$ for $c\ne0$, making $\exV{WW'}$ invertible, and implying the result we wanted.

\subsection*{b}

Due to the first-order condition, $-2\exV{X(Y-X'\beta)}=0$ (and the assumptions that $\exV{XX'}$ and $\exV{XY}$ exist, and that there is no perfect colinearity in $X$), we have that 

\begin{equation}
\beta=\exV{XX'}^{-1}\exV{XY}.
\end{equation}

Similarly for $BLP(Y|W)$, the first order condition is $-2\exV{W(Y-W'\gamma)}=0$, which - using the no perfect colinearity of $W$ and if $\exV{WW'}=A\exV{XX'}A'$ and $\exV{WY}=A\exV{XY}$ both exist (which is the case if $A$ has only finite real values, since $\exV{XX'}$ and $\exV{XY}$ exist) -  gives us that:
\begin{equation}
\begin{split}
\gamma&=\exV{WW'}^{-1}\exV{WY}=(A\exV{XX'}A')^{-1}A\exV{XY}\\
&=A'^{-1}\exV{XX'}^{-1}A^{-1}A\exV{XY}=A'^{-1}\beta
\end{split}
\end{equation}

\subsection*{c}

Define $\textbf{W}:=[W'_i]$, a matrix with vectors $W'_i$ as rows $(1\le i \le n)$, where $W'_i=X'_iA'$, so that $\textbf{W}=\textbf{XA'}$. Similarly, $\textbf{X}:=[X'_i]$ and $\textbf{Y}=[Y'_i]$. Then our estimates of $\beta$ and $\gamma$ using OLS are:
\begin{equation}
\begin{split}
\hat{\beta}_n&=(\textbf{X'X})^{-1}\textbf{X'Y}\\
\hat{\gamma}_n&=(\textbf{W'W})^{-1}\textbf{W'Y}=(\textbf{AX'XA'})^{-1}\textbf{AX'Y}\\
&=\textbf{A'}^{-1}\textbf{(X'X)}^{-1}\textbf{A}^{-1}\textbf{AX'Y}=\textbf{A'}^{-1}\hat{\beta}_n
\end{split}
\end{equation}

The conditionsl variance of the $\hat{\beta}_n$:
\begin{equation}
\begin{split}
Var(\hat{\beta}_n|\textbf{X})&=Var((\textbf{X'X})^{-1}\textbf{X'Y}|\textbf{X})\\
&=Var((\textbf{X'X})^{-1}\textbf{X'}(\textbf{X}\beta+\textbf{u})|\textbf{X})\\
&=Var((\textbf{X'X})^{-1}\textbf{X'}\textbf{u}|\textbf{X})\\
&=(\textbf{X'X})^{-1}\textbf{X'}Var(\textbf{u}|\textbf{X})\textbf{X}(\textbf{X'X})^{-1}\\
\end{split}
\end{equation}

First notice that, because $\textbf{A}$ is invertible, there is a one-to-one relation between $\textbf{W}$ and $\textbf{X}$. That is, given $\textbf{X}$, we know $\textbf{W}=\textbf{XA'}$, and given $\textbf{W}$, we know $\textbf{X}=\textbf{W}(\textbf{A'}^{-1})$. They both have the same information.

 Therefore, since any function of $\textbf{W}$ can be written as a function of $\textbf{X}$ and vice-versa, the space of functions of $\textbf{W}$ is the same as the space of functions of $\textbf{X}$. Then, if a function of $\textbf{W}$ is the conditional expectation $\exV{\hat{\gamma}_n^2|\textbf{W}}$, then the same function is also $\exV{\hat{\gamma}_n^2|\textbf{X}}$. Similarly we have $(\exV{\hat{\gamma}_n|\textbf{W}})^2=(\exV{\hat{\gamma}_n|\textbf{X}})^2$. Thus, because $Var(\hat{\gamma}_n|\textbf{W})=\exV{\hat{\gamma}_n^2|\textbf{W}}-(\exV{\hat{\gamma}_n|\textbf{W}})^2$,  we have $Var(\hat{\gamma}_n|\textbf{W})=Var(\hat{\gamma}_n|\textbf{X})$.

And since $\hat{\gamma}_n=\textbf{A'}^{-1}\hat{\beta}_n$, we have:
\begin{equation}
\begin{split}
Var(\hat{\gamma}_n|\textbf{W})=Var(\hat{\gamma}_n|\textbf{X})&=Var(\textbf{A'}^{-1}\hat{\beta}_n|\textbf{X})\\
&=\textbf{{A'}}^{-1}Var(\hat{\beta}_n|\textbf{X})\textbf{A}^{-1}\\
&=\textbf{{A'}}^{-1}(\textbf{X'X})^{-1}\textbf{X'}Var(\textbf{u}|\textbf{X})\textbf{X}(\textbf{X'X})^{-1}\textbf{A}^{-1}\\
\end{split}
\end{equation}


\section*{Question 6}

\subsection*{a}

Due to the first-order condition of $BLP(Y_i|W_i)$, we have $\exV{W_iU_i}=0$, which is equivalent to $\exV{U_i}=0$, $\exV{X_iU_i}=0$ and $\exV{Z_iU_i}=0$. Therefore, we have that $Cov(U_i,W_i)=\exV{U_iW_i}-\exV{U_i}\exV{W_i}=0$ and thus $W_i$ and $U_i$ are uncorrelated.

In this case, they are also mean independent. Since $\exV{Y_i|W_i}=W'_i\beta$ is the best predictor of $Y_i$ and it is also linear, then $BLP(Y_i|W_i)=W'_i\beta$, and we have $Y_i=W'_i\beta+U_i$. Taking conditional expectations we get:
\begin{equation}
\exV{Y_i|W_i}=W'_i\beta+\exV{U_i|W_i} \Longrightarrow \exV{U_i|W_i}=0.
\end{equation}

Thus $U_i$ is mean independent of $W_i$. This is due to, in this case, the best linear predictor being actually equal to the conditional expectation. 

\subsection*{b}

Since, as seen in letter (a) above, $Y_i=W'_i\beta+U_i$, we have that:
\begin{equation}
Var(U_i|W_i)=Var(Y_i-W'_i\beta|W_i)=Var(Y_i|W_i)
\end{equation}

Also, we have that $Var(U_i|W_i)=\exV{U_i^2|W_i}-(\exV{U_i|W_i})^2=\exV{U_i^2|W_i}$, since we have shown $\exV{U_i|W_i}=0$ above. Homoskedasticity would mean both $\exV{U_i|W_i}=0$ and $\exV{U_i^2|W_i}$ not depending on $W_i$.

Because $\exV{Y_i|W_i}=W'_i\beta$, and $Y_i$ takes values in $\{0,1\}$, we then have $Y_i|W_i$ distributed as bernoulli with $p=W'_i\beta$. This implies that $Var(U_i|W_i)=Var(Y_i|W_i)=W'_i\beta(1-W'_i\beta)$. Thus $Var(U_i|W_i)=\exV{U_i^2|W_i}$ depends on $W_i$, unless $\beta=0$, making it unreasonable to assume homoskedasticity, since this would imply $W_i$ is not useful in predictiong $Y_i$, and therefore our model is flawed from the start.

\subsection*{c}

Define $W:=[W'_i]$, that is, a matrix with $W'_i$ as its rows ($1\le i\le n$). Similarly define $Y:=[Y_i]$. With that, the OLS estimator of $\beta$ is:
 \begin{equation}
\hat{\beta}_n=(W'W)^{-1}W'Y
\end{equation}

Now take the conditional expectation of $\hat{\beta}_n$:
\begin{equation}
\exV{\hat{\beta}_n|W}=\exV{(W'W)^{-1}W'(W\beta+U)|W}=\beta+(W'W)^{-1}W'\exV{U|W}=\beta
\end{equation}

\noindent the last equality being due to $\exV{U|W}=0$, since $\exV{U_i|W_i}=0$ and the observations are i.i.d.. Thus we have conditional unbiasedness of $\hat{\beta}_n$. Applying the Law of Iteraded expectations we obtain unconditional as well: $\exV{\hat{\beta}_n}=\exV{\exV{\hat{\beta}_n|W}}=\exV{\beta}=\beta$.

\subsection*{d}

We can use CLT, WLLN and CMT to show:

\begin{equation}
\begin{split}
\sqrt{n}(\hat{\beta}_n-\beta)&=\sqrt{n}\Big(\big(\frac{1}{n}\sum_{i=1}^{n}{W_iW'_i}\big)^{-1}\big(\frac{1}{n}\sum_{i=1}^{n}{W_iY_i}\big)-\beta\Big)\\
&=\sqrt{n}\Big(\big(\frac{1}{n}\sum_{i=1}^{n}{W_iW'_i}\big)^{-1}\big(\frac{1}{n}\sum_{i=1}^{n}{W_i(W'_i\beta+U_i)}\big)-\beta\Big)\\
&=\sqrt{n}\Big(\beta+\big(\frac{1}{n}\sum_{i=1}^{n}{W_iW'_i}\big)^{-1}\big(\frac{1}{n}\sum_{i=1}^{n}{W_iU_i}\big)-\beta\Big)\\
&=\Big(\frac{1}{n}\sum_{i=1}^{n}{W_iW'_i}\Big)^{-1}\Big(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}{W_iU_i}\Big)\\
&\overset{d}{\to}N(0,\exV{W_iW'_i}^{-1}Var(W_iU_i)\exV{W_iW'_i}^{-1})=N(0,\Omega),
\end{split}
\end{equation}
\noindent where we define $\Omega$ accordingly. The last result is due to: (i) $\Big(\frac{1}{n}\sum_{i=1}^{n}{W_iW'_i}\Big)^{-1}\plim\exV{W_iW'_i}^{-1}$ due to WLLN and CMT; (ii) $\Big(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}{W_iU_i}\Big)\overset{d}{\to}N(0,Var(W_iU_i))$ due to CLT and $\exV{W_iU_i}=0$, as we have shown above; (iii) using slutsky and the fact that $\exV{W_iW'_i}^{-1}$ is symmetric we get the final result.

Because it is not reasonable to assume homoskedasticity, a consistent estimate of $\Omega$ would be:
\begin{equation}
\hat{\Omega}_n=\Big(\frac{1}{n}\sum_{i=1}^{n}{W_iW'_i}\Big)^{-1}\Big(\frac{1}{n}\sum_{i=1}^{n}{W_iW'_i\hat{U}^2_i}\Big)\Big(\frac{1}{n}\sum_{i=1}^{n}{W_iW'_i}\Big)^{-1},
\end{equation}
\noindent where $\hat{U}$ are the residuals, since we do not know the true errors. In words, we are substituting the terms in $\Omega$ by sample analogs. In class we proved this leads to a consistent estimator. We will use this fact below.

Now, using the CMT and $r:=\begin{pmatrix}
0&0&1\\
\end{pmatrix}'$ we have:
\begin{equation}
\begin{split}
\sqrt{n}(r'\hat{\beta}_{n}-r'\beta)&\overset{d}{\to}N(0,r'\Omega r)\\
\sqrt{n}(\hat{\beta}_{n,2}-\beta_2)&\overset{d}{\to}N(0,\begin{pmatrix}
0&0&1\\
\end{pmatrix}
\Omega\begin{pmatrix}
0\\
0\\
1\\
\end{pmatrix})=N(0,\Omega_{3,3}),
\end{split}
\end{equation}
\noindent where $\Omega_{3,3}$ is the $(3,3)$ element of the matrix $\Omega$.

Because the function that maps from a vector to a coordinate is continuous, we can use the CMT to conclude that $\hat{\Omega}_{3,3}\plim\Omega_{3,3}$, where $\hat{\Omega}_{3,3}$ is the $(3,3)$ element of the matrix $\hat{\Omega}_n$. Again by the CMT we have $\sqrt{\hat{\Omega}_{3,3}}\plim\sqrt{\Omega_{3,3}}$.

Using Slutsky (and CMT when applying the absolute value function), we then have:
\begin{equation}
\begin{split}
\frac{\sqrt{n}(|\hat{\beta}_{n,2}-\beta_2|)}{\sqrt{\hat{\Omega}_{3,3}}}&\overset{d}{\to}|N(0,1)|,
\end{split}
\end{equation}

Then, to test the null $H_0: \beta_2=0$, we could use the test statistic $T_n:=\frac{\sqrt{n}(|\hat{\beta}_{n,2}|)}{\sqrt{\hat{\Omega}_{3,3}}}$, and reject the null if $T_n>z_{1-\frac{\alpha}{2}}$. Then we have, using Portmanteau:
\begin{equation}
\begin{split}
\limsup{\Pr(T_n>z_{1-\frac{\alpha}{2}})}&\le \limsup{\Pr(Tn\ge z_{1-\frac{\alpha}{2}})}\\
&\le \Pr(|z|\ge z_{1-\frac{\alpha}{2}})=\alpha,
\end{split}
\end{equation}
\noindent where $|z|$ is standard normal, and $z_{1-\frac{\alpha}{2}}$ the $1-\frac{\alpha}{2}$ quantile. Thus, we have a test consistent at level $\alpha$.

\subsection*{e}

We know that, because the regression without $Z_i$ still has a constant, the estimate for $\beta_1$ would be (where variables with overlines are sample means):
\begin{equation}
\begin{split}
\hat{\beta}_{n,1}&=\big(\frac{1}{n}\sum_{i=1}^{n}{(X_i-\overline{X}_n)^2}\big)^{-1}\big(\frac{1}{n}\sum_{i=1}^{n}{(X_i-\overline{X}_n)(Y_i-\overline{Y}_n)}\big)\\
&=\big(\frac{1}{n}\sum_{i=1}^{n}{(X_i-\overline{X}_n)^2}\big)^{-1}\big(\frac{1}{n}\sum_{i=1}^{n}{(X_i-\overline{X}_n)(\beta_0+X_i\beta_1+Z_i\beta_2+U_i-\beta_0-\overline{X}_n\beta_1-\overline{Z}_n\beta_2-\overline{U}_n)}\big)\\
&=\big(\frac{1}{n}\sum_{i=1}^{n}{(X_i-\overline{X}_n)^2}\big)^{-1}\big(\frac{1}{n}\sum_{i=1}^{n}{(X_i-\overline{X}_n)((X_i-\overline{X}_n)\beta_1+(Z_i-\overline{Z}_n)\beta_2+(U_i-\overline{U}_n))}\big)\\
&=\beta_1+\beta_2\frac{\big(\frac{1}{n}\sum_{i=1}^{n}{(X_i-\overline{X}_n)(Z_i-\overline{Z}_n)}\big)}{\big(\frac{1}{n}\sum_{i=1}^{n}{(X_i-\overline{X}_n)^2}\big)}+\frac{\big(\frac{1}{n}\sum_{i=1}^{n}{(X_i-\overline{X}_n)(U_i-\overline{U}_n)}\big)}{\big(\frac{1}{n}\sum_{i=1}^{n}{(X_i-\overline{X}_n)^2}\big)}
\end{split}
\end{equation}

We know the last term in the last line converges in probabiliy to $\frac{Cov(X_i,U_i)}{Var(X_i)}=\frac{\exV{(X_i-\exV{X_i})(U_i-\exV{U_i})}}{\exV{(X_i-\exV{X_i})^2}}$ due to WLLN and CMT, since observations are i.i.d.. But we also know, from item (a) above, that $Cov(X_i,U_i)=0$; thus, the last term is converging in probability to zero. Also, again using WLLN and CMT, since observations are i.i.d., we have that the second fraction in the last line in converging in probability to $\beta_2\frac{Cov(X_i,Z_i)}{Var(X_i)}=\beta_2\frac{\exV{(X_i-\exV{X_i})(Z_i-\exV{Z_i})}}{\exV{(X_i-\exV{X_i})^2}}$.

Therefore, for $\hat{\beta}_{n,1}\plim\beta_1$ to hold, we need either $\beta_2=0$ or $Cov(X_i,Z_i)=0$. That is, for the estimate of $\beta_1$ omitting $Z_i$ to be consistent, we need $X_i$ and $Z_i$ to be uncorrelated.

\subsection*{f}

Using $\hat{\beta}_n=(W'W)^{-1}W'Y$, we have that:
\begin{equation}
Var(\hat{\beta}_n|W)=(W'W)^{-1}W'Var(U|W)W(W'W)^{-1}
\end{equation}

We know $Var(U|W)$ is a matrix with diagonal elements equal to $W'_i\beta(1-W'_i\beta)$, and off-diagonal elements zero, because the observations are i.i.d.. We do not know $\beta$, but we can proceed by first obtaining an OLS estimate $\hat{\beta}_n=(W'W)^{-1}W'Y$. Then we use this OLS estimate to estimate $Var(U|W)$ by the matrix $\hat{\Omega}_n$ that has $W'_i\hat{\beta}_n(1-W'_i\hat{\beta}_n)$ in its diagonals and zero off-diagonals. Because OLS is consistent, using the CMT we obtain that $\hat{\Omega}_n\plim Var(U|W)$.

Now we reestimate $\beta$ using $\hat{\beta}_n^{*}=(W'\hat{\Omega}_n^{-1}W)^{-1}W'\hat{\Omega}_n^{-1}Y$. This gives us that:
\begin{equation}
\begin{split}
Var(\hat{\beta}_n^{*}|W)&=(W'\hat{\Omega}_n^{-1}W)^{-1}W'\hat{\Omega}_n^{-1}Var(U|W)\hat{\Omega}_n^{-1}W(W'\hat{\Omega}_n^{-1}W)^{-1}\\
&\plim (W'\Omega^{-1}W)^{-1}W'\Omega^{-1}\Omega\Omega^{-1}W(W\Omega^{-1}W)^{-1}=(W\Omega^{-1}W)^{-1}
\end{split}
\end{equation}

Thus, by the results of question 2, using $\Omega$ as our $D$, we obtain an estimator whose variance converges to the best variance possible among unbiased estimators, in the gauss-markov sense.


\end{document}
