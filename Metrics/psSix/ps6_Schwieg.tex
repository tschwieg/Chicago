\documentclass[12pt]{paper}

\usepackage{Schwieg}
\usepackage[margin=1in]{geometry}
\usepackage{minted}

\begin{document}

\section{Question 1}


Conditioned upon $X$, $Y_i \sim \normal( X_i' \theta, \sigma^2)$. This tells us
that $\frac{Y_i - X_i' \theta}{\sigma} \sim \normal( 0, 1)$. As a result, the
density of $Y_i$ given $X_i$ is given by:
$\frac{1}{\sigma}\phi( \frac{z - X_i' \theta}{\sigma})$.



\subsection{What is the (condition) likelihood function? What is the
  (conditional) log-likelihood function?}

\begin{align*}
  \ell_n(\theta) &= \prod_{i=1}^n p_{\theta}( y_i \vert x_i)\\
         &= \prod_{i=1}^n \frac{1}{\sigma} \phi\left( \frac{ y_i - x_i' \theta}{\sigma}\right)\\
  &= \frac{1}{\sigma^n} \prod_{i=1}^n \phi \left( \frac{y_i - x_i' \theta}{\sigma}\right)
\end{align*}

\begin{align*}
  L_n(\theta) = - \log( \sigma ) + \frac{1}{n} \sum_{i=1}^n \log \left( \phi \left(
  \frac{y_i - x_i'\theta}{\sigma} \right) \right)
\end{align*}

\subsection{Find the ML estimators}

\begin{align*}
  \deriv{L_n(\theta)}{\theta_j} &= \frac{1}{n} \sum_{i=1}^n \frac{\phi' \left( \frac{y_i
  - x_i' \theta}{\sigma} \right) ( - x_{i,j})}{\phi \left( \frac{y_i - x_i'\theta}{\sigma}
                        \right)} = 0\\
  \deriv{L_n(\theta)}{\sigma} &= \frac{-1}{\sigma} + \frac{1}{n} \sum_{i=1}^n\frac{\phi' \left( \frac{y_i
  - x_i' \theta}{\sigma} \right) \left( \frac{-(y_i - x_i' \theta)}{\sigma^2} \right)}{\phi \left( \frac{y_i - x_i'\theta}{\sigma}
                      \right)} = 0\\
\end{align*}

Let us simplify the notation by using
\begin{align*}
  A_i &= \frac{\phi' \left( \frac{y_i
  - x_i' \theta}{\sigma} \right) }{\phi \left( \frac{y_i - x_i'\theta}{\sigma}
      \right)} = \frac{y_i - x_i' \theta}{\sigma}\\
  0 &= \frac{1}{n} \sum_{i=1}^n \frac{y_i - x_i' \theta}{\sigma} ( - x_{i,j})\\
  \\
  \sigma &= \frac{1}{n} \sum_{i=1}^n \frac{y_i - x_i' \theta}{\sigma} ( y_i - x_i' \theta)\\
  \sigma^2 &= \frac{1}{n} \sum_{i=1}^n u_i^2\\
  0 &= \sum_{i=1}^n  x_{i,j} u_i \quad \forall j\\
\end{align*}

Let us denote the matrix containing $X_i$ on the $i^{th}$ row as
$\bm{X}$. Then our condition simplifies to:
\begin{equation*}
    0 = \bm{X} \bm{u}
\end{equation*}

This is exactly the orthogonality condition found in linear
regression, and as such has the same solution.

\begin{align*}
  \est{\theta} &= \inv{(X' X)} X'Y\\
  \est{\sigma^2} &= \frac{1}{n}\sum_{i=1}^n u_i^2
\end{align*}

\subsection{(c)}

Consider testing $H_0 : \theta = \theta_0$ vs. $H_1: \theta \neq \theta_0$. Describe how you
would carry this out using each of the three hypothesis testing
methods described in class. How do they compare with the Wald tests
and LM tests described earlier in class using ordinary least squares?\par

Let $f(\theta)$ denote the log-likelihood function evaluated at $\theta$. Let
$g(\theta)$ denote the gradient of the log-likelihood function evaluated at
$\theta$. 
\begin{itemize}
\item Wald Test\newline

Let us calculate an estimator of the variance of $\est{\theta}$,
$\est{\Omega}$. We may estimate this using an estimate of the outer-product
gradient of the log-likelihood function.

\begin{equation*}
  \est{\Omega} = \frac{1}{n} \sum_{i=1}^n \nabla L_n(\est{\theta}) \nabla L_n( \est{\theta})
\end{equation*}

Then a Wald-Test would use the fact that $\sqrt{n} ( \est{\theta} - \theta_0)
\convDist \normal( 0, \est{\Omega})$. As $\sqrt{n} ( \est{\theta} - \theta_0)
\convDist \normal( 0, \Omega)$ and $\est{\Omega} \plim \Omega$. By our choice of
construction of $\est{\Omega}$, it is positive definite, and therefore has
a choleksy decomposition. Then we may write $\est{\Omega} = \est{F}
\est{F}'$ where $\est{F}$ is a lower-triangular matrix. 

This test would take the form of $\indicate{ T_n > z_{1-\alpha}}$ where
$T_n := \est{F} \sqrt{n}( \est{\theta} - \theta_0)$, and $z_{1-\alpha}$ is the critical value
for the normal distribution.

The question of interest is whether or not $\est{\Omega} \plim \sigma^2
\inv{(X'X)}$, the variance estimator used in OLS.

\begin{align*}
  \est{\Omega} &\plim \Omega\\
\end{align*}




\item LR Test\newline

Let us define a new estimator of the variance, $\altest{\sigma^2}$, which is
our estimate of the variance when we hold $\theta$ fixed at $\theta_0$.

\begin{equation*}
  \altest{\sigma^2} = \frac{1}{n} \sum_{i=1}^n \hat{u}_i^2
\end{equation*}

Define the log-likelihood function as $f( \theta, \sigma^2 )$. We wish to
compare the constrained log-likelihood, $f(\theta_0, \altest{\sigma^2})$ to the
unconstrained log-likelihood: $f( \est{\theta}, \est{\sigma^2})$.

It is known that
\begin{equation*}
  2( f( \est{\theta}, \est{\sigma^2}) - f( \theta_0, \altest{\sigma^2})) \convDist \chi^2(k+1)
\end{equation*}

Therefore we can define a test to take the form of $\indicate{T_n >
  c_{1-\alpha}}$ where $T_n := 2( f( \est{\theta}, \est{\sigma^2}) - f( \theta_0,
\altest{\sigma^2}))$ and $c_{1-\alpha}$ is the critical value of a chi-squared
distribution with $k+1$ degrees of freedom.

We have not examined an analog for this test in OLS.
 
  
  
\end{itemize}

\section{Question 2}

Let $X_1,...,X_n$ be an i.i.d. sequence of random variables with
distribution $\unif(\theta,2\theta)$ with $\theta > 0$ i.e. uniformly distributed on
the interval $[\theta,2\theta]$.

\subsection{(a)}

Show that the maximum likelihood estimator of $\theta$ is given by
\begin{equation*}
  \est{\theta} = \frac{1}{n} \max_{1 \leq i \leq n} X_i
\end{equation*}

The density of $X_i$ is given by: $\frac{1}{\theta} \indicate{ X_i \in
  [\theta,2\theta]}$. The likelihood function is then given by:

\begin{align*}
  \ell_n(\theta) &= \prod_{i=1}^n \frac{1}{\theta} \indicate{ X_i \in [\theta,2\theta]}\\
  &= \frac{1}{\theta^n} \indicate{ X_i \in [\theta,2\theta] \quad \forall i}
\end{align*}

The log-likelihood function is then given by: 
\begin{equation*}
  L_n(\theta) = -n \log( \theta) + \log( \indicate{ X_i \in [\theta,2\theta] } )  
\end{equation*}

This function is decreasing in $\theta$, provided that all $X_i \in
[\theta,2\theta]$. This means that we wish to choose the smallest possible $\theta$
value that maintains all $\theta \leq X_i \leq 2\theta$. The upper bound is the
relevant bound, so we allow for $\est{\theta} = \frac{1}{2} \max_{1\leq i \leq
  n}X_i$. 

\subsection{(b)}

Let us determine the distribution of $Z = \max X_i$.
Let $z \in [\theta,2\theta]$. 
\begin{align*}
  \Pr( Z \leq z ) &= \Pr( \max X_i \leq z)\\
               &= \cap \Pr( X_i \leq z)\\
               &= \left(  \frac{z - \theta}{\theta} \right)^n\\
  f_z(z) &= n \left( \frac{z-\theta}{\theta} \right)^{n-1} \frac{1}{\theta}\\
  \exV{Z} &= \frac{n}{\theta}\int_{\theta}^{2\theta} z \left(  \frac{z}{\theta} - 1
            \right)^{n-1} dz\\
               &= n \int_0^1 (\theta u + \theta )u^{n-1}du\\
               &= n \int_0^1 \theta u^n + \theta u^{n-1}du\\
  &= \theta \frac{2n + 1}{n+1} 
\end{align*}

From this we can see that:
\begin{equation*}
  \exV{ \est{\theta}} = \frac{1}{2} \exV{ \max{X_i}} = \theta \frac{2n+1}{2n+2}
  \neq \theta
\end{equation*}

So $\est{\theta}$ is not an unbiased estimator of $\theta$.

\subsection{(c)}

Prove that $\est{\theta}$ is a consistent estimator of $\theta$.

\begin{align*}
  \Pr( \abs{\est{\theta} - \theta} < \epsilon) &= \Pr( \abs{ \frac{1}{2}\max{X_i} - \theta}
                                < \epsilon)\\
                              &= \Pr( \theta - \frac{1}{2} \max{x_i} < \epsilon)\\
                              &= \Pr( \max{X_i} > 2( \theta - \epsilon))\\
                              &= 1 - \Pr( \max{X_i} < 2( \theta - \epsilon))\\
                              &1 - \left(  \frac{\theta - 2 \epsilon}{\theta}\right)^n\\
  &\rightarrow 1
\end{align*}

\subsection{(d)}

Show that $n( \theta - \est{\theta})$ converges in distribution to an exponential
distribution with parameter $\lambda$ as $n\rightarrow\infty$. Express $\lambda$ in terms of $\theta$.

Firstly we can note that the Probability that $\theta - \est{\theta} < 0$ is
zero. There is no support for $X_i > \theta$, so the maximum cannot be
greater than $\theta$.  

\begin{align*}
  \Pr( n ( \theta - \est{\theta}) \leq x) &= \Pr( \theta - \est{\theta} \leq \frac{x}{n})\\
                             &= 1 - \left( \frac{\theta - \frac{2x}{n}}{\theta}
                               \right)^n\\
                             &= 1 - \left(  1 - \frac{2x}{n\theta} \right)^n\\
  &\rightarrow 1 - \exp( - \frac{2x}{\theta})
\end{align*}

If we let $\lambda = \frac{\theta}{2}$, then we see that $n( \theta - \est{\theta})
\convDist \exp( \frac{\theta}{2})$.

\subsection{(e)}

Construct an (approximate) 95\% confidence interval for $\theta$. Justify
your answer.

We are certain that $\theta > \frac{1}{2}\max{X_i}$, so this is a logical
lower bound for our interval. We wish to use part (d) to determine the
upper bound.

We are interested in an interval $C_n := [\frac{1}{2} \max{X_i}, c]$ such
that $\Pr( \theta \in C_n ) \leq .95$.
\begin{align*}
  \Pr( \theta < c ) &= \Pr( n\theta < nc + n \est{\theta} - n \est{\theta})\\
               &= \Pr( n( \theta - \est{\theta}) < n \left( c - \est{\theta}\right)
                 )\\
               &\rightarrow 1 - \exp \left( - \frac{2n ( c - \est{\theta})}{\theta} \right)\\
  &\leq 1 - \exp \left( - \frac{2n ( c - \est{\theta})}{\est{\theta}} \right)
\end{align*}

Since we know that $\est{\theta} < \theta$. Setting this probability equal to
$.95$.

\begin{align*}
  1 - \exp \left( - \frac{2n ( c - \est{\theta})}{\est{\theta}} \right) &= .95\\
  \exp \left( - \frac{2n ( c - \est{\theta})}{\est{\theta}} \right) &= .05\\
  \frac{2n ( c - \est{\theta})}{\est{\theta}} &= - \log .05\\
  c &= \est{\theta} - \frac{\est{\theta}\log(.05)}{2n}
\end{align*}

From this we know that: $\Pr( \theta \in [\est{\theta}, c] ) \leq .95$. as $n \rightarrow \infty$.

\section{Question 3}

\subsection{(a)}

Suggest an unbiased estimator $\altest{\theta}$ of $\theta$. Justify your
answer.

Using a Method of Moments estimator, we may set the sample mean equal
to the population mean.

\begin{align*}
  \frac{1}{n} \sum_{i=1}^n X_i &= \exV{X}\\
  \mean{X} &= \frac{3 \theta}{2}\\
  \altest{\theta} &= \frac{2 \mean{X}}{3}
\end{align*}


To show this is an unbiased estimator:

\begin{align*}
  \exV{\altest{\theta}} &= \frac{2}{3}\exV{ \mean{X}}\\
                   &= \frac{2}{3} \frac{3\theta}{2}\\
                   &= \theta
\end{align*}


\begin{minted}[mathescape,
               linenos,
               breaklines,
               numbersep=5pt,
               frame=lines,
               firstnumber=last]{julia}
using Random
using Latexify
using Distributions

thetapos = [0.5,1,10]
npos = [2,5,20,100]

Random.seed!( 235711 )

#This just allocates a bunch of arrays to fit all the data we're asked to simulate
X = zeros( length(thetapos), length(npos) )
hatABS = zeros( length(thetapos), length(npos) )
tildeABS = zeros( length(thetapos), length(npos) )
hatMSE = zeros( length(thetapos), length(npos) )
tildeMSE = zeros( length(thetapos), length(npos) )
ind = zeros( length(thetapos), length(npos) )

tempx = zeros(10000,maximum(npos))
tempHatTheta = zeros(10000)
tempTildeTheta = zeros(10000)
    
for i in 1:length(thetapos)
    theta = thetapos[i]
    dist = Uniform( theta, 2*theta)
    for j in 1:length(npos)
        n = npos[j]
        for k in 1:10000
            tempx[k,1:n] = rand( dist, n)
            tempHatTheta[k] = maximum(tempx[k,1:n])/2.0
            tempTildeTheta[k] = (2.0/3.0)*mean(tempx[k,1:n])
        end

        tempHatABS = abs.( tempHatTheta .- theta)
        hatABS[i,j] = mean( tempHatABS )

        tempTildeABS = abs.( tempTildeTheta .- theta)
        tildeABS[i,j] = mean(tempTildeABS)

        tempHATMSE = tempHatABS .* tempHatABS
        hatMSE[i,j] = mean(tempHATMSE)

        tempTildeMSE = tempTildeABS .* tempTildeABS
        tildeMSE[i,j] = mean( tempTildeMSE)

        temp = tempTildeMSE .> tempHATMSE
        ind[i,j] = mean(temp)
    end
end
latexify( hatABS )
latexify( tildeABS )
latexify( hatMSE )
latexify( tildeMSE )
latexify( ind )
\end{minted}

Let the rows of the tables vary on $\theta$, and the columns to vary on $n$.

\begin{equation}
  \exV{ \abs{ \est{\theta} - \theta}} = 
\left[
\begin{array}{cccc}
0.08406155 & 0.04192225 & 0.01200721 & 0.00244757 \\ 
0.16782184 & 0.08347129 & 0.02412170 & 0.00491011 \\ 
1.65429116 & 0.83625597 & 0.23698989 & 0.04959539 \\ 
\end{array}
\right]
\end{equation}


\begin{equation}
  \exV{ \abs{ \altest{\theta} - \theta}} = 
\left[
\begin{array}{cccc}
0.055846444 & 0.03451907 & 0.01698833 & 0.00760587\\ 
0.111192510 & 0.06964597 & 0.03418168 & 0.01506232\\ 
1.119239886 & 0.69265960 & 0.34332960 & 0.15368077\\ 
\end{array}
\right]
\end{equation}


\begin{equation}
  \exV{ \left( \est{\theta} - \theta \right)^2} = 
\left[
\begin{array}{cccc}
0.01024438 & 0.00291720 & 0.00027466 & 1.18647824 \times 10^{-5} \\ 
0.04150057 & 0.01226166 & 0.00110024 & 4.83207145 \times 10^{-5} \\ 
4.13854889 & 1.18343059 & 0.10500403 & 0.00480661 \\ 
\end{array}
\right]
\end{equation}


\begin{equation}
  \exV{ \left( \altest{\theta} - \theta \right)^2} = 
\left[
\begin{array}{cccc}
0.00466503 & 0.0018422 & 0.00045143 & 9.03422956 \times 10^{-5} \\ 
0.01849642 & 0.0074670 & 0.00181460 & 0.00036373 \\ 
1.86594666 & 0.7375063 & 0.18430868 & 0.03693727 \\ 
\end{array}
\right]
\end{equation}


\begin{equation}
  \Pr( \abs{ \altest{\theta} - \theta} < \abs{ \est{\theta} - \theta} )
\left[
\begin{array}{cccc}
0.292 & 0.4114 & 0.6225 & 0.8097 \\ 
0.2904 & 0.4081 & 0.6311 & 0.7977 \\ 
0.2952 & 0.4149 & 0.6242 & 0.8045 \\ 
\end{array}
\right]
\end{equation}

We can see that at very small values of $n$, unbiasedness is a
valuable property, but as soon as we get to reasonable data sizes, the
maximum likelihood estimator outperforms the Method of Moments
estimator by a large margin. So for data sets of reasonable size, I
would always prefer the maximum likelihood estimator for Uniform
estimation.

Unbiasedness is not always the most important property. Maximum
likelihood estimators trade off bias for much lower variance. This
trade-off can be examined by looking at Mean-Squared error, which is
considers this trade-off.

\subsection{(c)}

Can you justify your preference on theoretical grounds?

From the Central Limit Theorem, we know that $\sqrt{n} ( \altest{\theta} -
\theta) \sim \normal$. So we know that $\altest{\theta}$ is $\sqrt{n}$-consistent
for $\theta$. However, $\est{\theta}$ is $n$-consistent for $\theta$. This indicates
that $\est{\theta}$ is converging to $\theta$ more quickly than $\altest{\theta}$
is. We are able to ``blow up'' $\est{\theta}$ by more than $\altest{\theta}$ and
still have the sequence be tight.

From part (d) of question 2, we showed that $\est{\theta}$ was
$n$-consistent, and that it converged to an exponential
distribution. This tells us that the order of convergence of the
maximum likelihood estimator is $\bigO( \frac{1}{n})$ rather than the
order of converge of $\bigO ( \frac{1}{\sqrt{n}})$ of the Method of
Moments estimator.

\section{Question 4}

\subsection{(a)}


\begin{align*}
  \ell_n(\theta) &= \prod_{i=1}^n p_{\theta}(y \vert x)\\
         &= \prod_{i=1}^n G(x'\theta)^y ( 1 - G(x'\theta))^{1-y}\\
  L_n(\theta) &= \frac{1}{n} \sum_{i=1}^n y \log( G(x'\theta) ) + (1-y) \log( 1 - G(x'\theta))
\end{align*}

\subsection{(b)}
Uniqueness of the Maximum likelihood estimator requires that
\begin{align*}
  \Pr( p_{\theta}( y \vert x) \neq p_{\theta_0}(y \vert x)) > 0\\
  \Pr( G( x'\theta) \neq G( x' \theta_0) > 0)\\
\end{align*}

This occurs when $G$ is a one-to-one function, and there is no perfect
colinearity in $X$. A class of functions with this property that also
ensures this is a valid probability measure is the class of strictly
increasing functions with range $[0,1]$.

\subsection{(c)}

No, there is perfect separation between the data, so the logit
regression will not reach a maximum.

We would like to predict the $y$ for any value of $x$ below $1$ as
$0$, and for any value of $x$ above $0.8$ as $1$. We can always choose
a steeper logit function to increase the probability that we are
correct for this data. This implies that there is no maximum, as the
likelihood function is unbounded above.

Assume that $x$ takes the form of $(1,X_i)'$ and that
$\theta = (a,b)'$ For any proposed maximum, $0 < G( x'\theta) < 1$. We also know
that $-\frac{a}{b}$ must be between $.8$ and $1$, as this is the point
of inflection. By increasing $b$, and ensuring that $-\frac{a}{b}$ is
constant between $.8$ and $1$, the probability of the $y=1$ data
increases, and the probability of the $y=0$ data increases as
well. This increases the likelihood function, and contradicts the
assumption that it was a maximum.

\subsection{(d)}

\begin{align*}
  \log p_{\theta}( y \vert x ) &= y \log( G(x'\theta) ) + (1-y) \log( 1 - G(x'\theta))\\
\deriv{ \log p_{\theta} (y \vert x)}{\theta_j} &= \frac{y G'( x'\theta) x_j}{G(x'\theta)} -
                                   \frac{(1-y)G'(x'\theta) x_j}{G(x'\theta)}\\
\nabla \log p_{\theta}( y \vert x) &= \frac{y G'(x'\theta) - (1-y)G'(x'\theta)}{G(x'\theta)} X\\
\nabla \log p_{\theta}( y \vert x) \nabla \log p_{\theta}( y \vert x)' &= \left( \frac{y G'(x'\theta) -
                                             (1-y)G'(x'\theta)}{G(x'\theta)}
                                             \right)^2 X X'
\\
\end{align*}

Call this outer-product matrix $A$. Under the appropiate regularity
conditions we know that $\sqrt{n}(\est{\theta} - \theta) \convDist \normal( 0,
\inv{A})$. 


\section{Question 5}

Let $X_i$ be an i.i.d. sequence of random variables with pdf on
$\setR$ given by:
\begin{equation*}
  f_{\theta}(x) =
  \begin{cases}
    (1+\theta)x^{\theta}\quad &\text{ for } 0 < x < 1\\
    0 & otherwise
  \end{cases}
\end{equation*}
for some $\theta > -1$.

\subsection{(a)}

\begin{align*}
  \mu = \exV{X_i} &= \int_0^1 (1+\theta)x^{\theta+1}\\
            &= \frac{1+\theta}{2+\theta} x^{\theta+2} \vert_0^1\\
  &= \frac{1+\theta}{2+\theta}  
\end{align*}

\begin{align*}
  \mu(2+\theta) &= 1 + \theta\\
  2\mu - 1 &= \theta( 1 - \mu)\\
  \theta &= \frac{2\mu - 1}{ 1 - \mu} \quad \mu \in (0,1)
\end{align*}

\subsection{(b)}

Write the log-likelihood function as a function of $\mu$.

\begin{align*}
  L_n (\theta) &= \frac{1}{n} \sum_{i=1}^n \log(1 + \theta) + \theta \log( X_i )\\
  L_n( \mu )&= \frac{1}{n} \sum_{i=1}^n \log\left( \frac{\mu}{1-\mu} \right)
            + \frac{2\mu - 1}{ 1 - \mu} \log( X_i )\\
  L_n( \mu )&= \frac{1}{n} \sum_{i=1}^n \log \mu - \log ( 1 - \mu) 
            + \frac{2\mu - 1}{ 1 - \mu} \log( X_i )
\end{align*}

\subsection{(c)}

We believe that the solution will be interior. This condition is
equivalent to saying that $\mu \neq 0$ and $\mu \neq 1$.

\begin{align*}
  0 = \deriv{L_n(\mu)}{\mu} &= \frac{1}{n} \sum_{i=1}^n \frac{\mu \log(X_i) - \mu +
                      1}{(1-\mu)^2\mu} = 0\\
  0 &= \frac{1}{n} \sum_{i=1}^n \mu \log(X_i) - \mu + 1\\
  \est{\mu} &= \frac{1}{1 - \frac{1}{n} \sum_{i=1}^n \log(X_i)}   
\end{align*}

As long as $\log(X_i) \neq 0$ and $\log(X_i)\neq 1$, which occurs with
probability $0$.

\subsection{(d)}

We may note that $\est{\mu}$ is a continuous function of $\frac{1}{n}
\sum_{i=1}^n \log( X_i)$. Provided that $\exV{ \log( X_i)}$ exists, the
Weak Law of Large Numbers tells us that $\frac{1}{n} \sum_{i=1}^n \log(
X_i) \plim \exV{ \log( X_i)}$.

\begin{align*}
  \exV{ \log( X_i)} &= \int_0^1 (1+\theta) \log( x ) x^{\theta}\\
                    &=  \log( x) x^{\theta+1} \vert_0^1 - \int_0^1 \frac{x^{\theta+1}}{x}dx\\
                    &= -\frac{x^{\theta+1}}{\theta+1} \vert_0^1\\
  &= - \frac{1}{\theta+1}
\end{align*}

By the Continuous mapping theorem, we know that:
\begin{align*}
  \est{\mu} &\plim \frac{1}{1 - \frac{-1}{\theta+1}}\\
          &= \frac{\theta+1}{\theta+2}\\
  &= \mu
\end{align*}

\subsection{(e)}

\begin{align*}
  \exV{\log^2(X_i)} &= \int_0^1 (1+\theta)\log^2(x ) x^{\theta}dx\\
                    &= \log^2(x) x^{\theta+1} \vert_0^1 - \int_0^1 2 \log(x) x^{\theta+1}dx\\
  &= - 2 \frac{1}{\theta+2} \log(x) x^{\theta+2} \vert_0^1 + \frac{2}{\theta+2} \int_0^1
    x^{\theta+1}dx\\
  &= \frac{2}{(\theta+2)^2}
\end{align*}

We notice that
\begin{equation*}
\Vari{\log(X_i)} = \exV{ \log(X_i)^2} - \exV{\log(X_i)}^2 = \frac{1}{(1+\theta)^2}
\end{equation*}

Since we know that the variance of $\log(X)$ is finite, we may apply
the central limit theorem.

\begin{equation*}
 \sqrt{n} ( \frac{1}{n} \sum_{i=1}^n \log( X_i) - \frac{-1}{1+\theta} )
 \convDist \normal( 0, \frac{1}{(1+\theta)^2})
\end{equation*}

Applying the delta-method, with the function $f(x) = \frac{1}{1-x}$.
Note that its derivative is: $g(x) = \frac{1}{(1-x)^2}$.

Thus: $g( \frac{-1}{1+\theta})^2 = \left( \frac{1}{(1-\frac{-1}{(1+\theta)})^2}
\right)^2 = \left(  \frac{\theta+1}{\theta+2} \right)^4$

\begin{equation*}
  \sqrt{n} ( \est{\mu} - \mu) \convDist \normal( 0,
  \frac{(\theta+1)^2}{(\theta+2)^4} ) = \normal( 0, \mu^2 ( 1 - \mu)^2)
\end{equation*}

\subsection{(f)}

The log-likelihood function for $\mu$ is given by:
\begin{equation*}
  log(\mu) - \log( 1 - \mu) + \frac{2\mu - 1}{ 1 - \mu} \log( X_i )
\end{equation*}

\begin{align*}
  g(\mu) &= \frac{1}{\mu} + \frac{1}{1-\mu} + \frac{1}{(1-\mu)^2} \log( X_i)\\
  h(\mu) &= \frac{-1}{\mu^2} + \frac{1}{(1-\mu)^2} + \frac{2}{(1-\mu)^3} \log(X_i)
\end{align*}

The Information matrix is given by the negative of the expected value of the second
derivative.

\begin{equation*}
  \exV{ \log( X_i) } = \frac{-1}{1+\theta} = \frac{-(\theta+2)}{\mu} = \frac{-(1-\mu)}{\mu}
\end{equation*}
Combining:
\begin{align*}
  \exV{ h(\mu)} &= \frac{-1}{\mu^2} + \frac{1}{(1-\mu)^2} + \frac{2}{(1-\mu)^3}
                \frac{-(1-\mu)}{\mu}\\
                &= \frac{-1}{\mu^2 (1-\mu)^2}
\end{align*}

Fischer's information matrix is therefore:
\begin{equation*}
  -B = \frac{1}{\mu^2 ( 1 - \mu)^2}
\end{equation*}
This is the inverse of the variance computed via the delta-method.


\subsection{(g)}

Since we have a one-dimensional estimate, our constrained estimate of
$\mu$ is $\altest{\mu} = \frac{2}{3}$.

We know that $\inv{H_n} \plim -B$ and $F_n \plim 1$. So we estimate
$\inv{H_n}$ with $\frac{1}{(\frac{2}{3})^2 (\frac{1}{3})^2} = \frac{81}{4}$.

The log-likelihood is given by: 
\begin{equation*}
  L_n( \mu )= \frac{1}{n} \sum_{i=1}^n \log \mu - \log ( 1 - \mu) 
  + \frac{2\mu - 1}{ 1 - \mu} \log( X_i )
 \end{equation*}

 Taking the derivative:

 \begin{equation*}
   \deriv{L_n}{\mu}( \altest{\mu}) = \frac{3}{2} + 3 +  9 \frac{1}{n} \sum_{i=1}^n \log X_i
 \end{equation*}

 The score test is based on the fact that

 \begin{equation*}
   F_n \inv{H_n} \sqrt{n} \deriv{L_n}{\mu}(\altest{\mu}) \convDist
   \normal( \inv{-B})
 \end{equation*}

 We then can construct a test based on the test statistic
 \begin{equation*}
   T_n := \frac{9}{2} \sqrt{n} \left(  \frac{9}{2} + 9 \frac{1}{n}
     \sum_{i=1}^n \log X_i \right)
 \end{equation*}

 We know that under the null, this test statistic is distributed
 $\normal(0,1)$ so we will use the test:

 \begin{equation*}
   \indicate{\abs{T_n} > z_{1-\frac{\alpha}{2}}}
 \end{equation*}

 where $z_{1-\frac{\alpha}{2}}$ is a critical value from a normal distribution.

\end{document}
